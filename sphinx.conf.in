#
# sphinx configuration file sample
#

#############################################################################
## data source definition
#############################################################################

source src1
{
	# data source type
	# for now, known types are 'mysql' and 'xmlpipe'
	# MUST be defined
	type				= mysql

	# whether to strip HTML
	# values can be 0 (don't strip) or 1 (do strip)
	# WARNING, only works with mysql source for now
	# WARNING, only works ok for PERFECTLY formed XHTML for now
	# WARNING, WILL BUG on normal everday HTML
	# optional, default is 0
	strip_html			= 0

	# what HTML attributes to index if stripping HTML
	# format is as follows:
	#
	# index_html_attrs	= img=alt,title; a=title;
	#
	# optional, default is to not index anything
	index_html_attrs	=

	#############################################################################

	# some straightforward parameters for 'mysql' source type
	sql_host			= localhost
	sql_user			= test
	sql_pass			=
	sql_db				= test
	# sql_sock			= /tmp/mysql.sock	# optional,
										# usually '/var/lib/mysql/mysql.sock' on Linux,
										# usually '/tmp/mysql.sock' on FreeBSD
	sql_port			= 3306				# optional, default is 3306


	# pre-query, executed before the main fetch query
	# useful eg. to setup encoding or mark records
	# optional, default is empty
	#
	# sql_query_pre		= SET CHARACTER_SET_RESULTS=cp1251
	sql_query_pre		=


	# main document fetch query
	# you can specify any number of fields
	#
	# document_id MUST be the very first field
	# document_id MUST be positive (non-zero, non-negative)
	# document_id MUST fit into 32 bits
	# document_id MUST be unique
	#
	# mandatory
	sql_query			= \
		SELECT id, group_id, UNIX_TIMESTAMP(date_added) AS date_added, title, content \
		FROM documents


	# query range setup
	#
	# useful to avoid MyISAM table locks and big result sets
	# when indexing lots of data
	#
	# to use query ranges, you should
	# 1) provide a query to fetch min and max id (ie. id range) from data set;
	# 2) configure step size in which this range will be walked;
	# 3) use $start and $end macros somewhere in the main fetch query.
	#
	# 'sql_query_range' must return exactly two integer fields
	# in exactly min_id, max_id order
	#
	# 'sql_range_step' must be a positive integer
	# optional, default is 1024
	#
	# 'sql_query' must contain both '$start' and '$end' macros
	# if you are using query ranges (because it obviously would be an
	# error to index the whole table many times)
	#
	# note that the intervals specified by $start/$end do not
	# overlap, so you should NOT remove document ids which are exactly
	# equal to $start or $end in your query
	#
	# here's an example which will index 'documents' table
	# fetching (at most) one thousand entries at a time:
	#
	# sql_query_range		= SELECT MIN(id),MAX(id) FROM documents
	# sql_range_step		= 1000
	# sql_query			= \
	#	SELECT doc.id, doc.id AS group, doc.title, doc.data \
	#	FROM documents doc \
	#	WHERE id>=$start AND id<=$end


	# group_id and date column names or numbers
	# these fields MUST be positive (non-zero, non-negative) integers
	# these fields MUST fit into 32 bits
	#
	# columns are numbered starting from 1, ie. document_id is 1, next one is 2, etc
	# if any of the columns is configured, it's removed from fulltext indexed fields,
	# therefore changing the field numbering
	#
	# for instance, in this example configuration, 'title' column will
	# number 1 and 'data' column will be number 2, because id/group/date
	# columns are removed
	#
	# optional, default is empty
	sql_group_column	= group_id
	sql_date_column		= date_added

	# an example of specifying group/timestamp columns by index:
	#
	# sql_group_column	= 2
	# sql_date_column	= 3


	# post-query, executed immediately after the main fetch query is over
	# useful eg. to unmark records
	# optional, default is empty
	sql_query_post		=


	# post-index-query, executed after the indexing is succesfully completed
	# $maxid macro is available, which is the max document ID actually fetched from DB
	# optional, default is empty
	#
	# sql_query_post_index = REPLACE INTO counters ( id, val ) VALUES ( 'max_indexed_id', $maxid )


	# document info query
	# optional, default is empty
	#
	# ONLY used by search utility to display document information
	# MUST fetch desired document field by its id, therefore must contain '$id' macro 
	#
	sql_query_info		= SELECT * FROM documents WHERE id=$id

	#############################################################################

	# demo config for 'xmlpipe' source type is a little below
	# with xmlpipe, sphinx opens a pipe to a given command, and reads documents from stdin
	#
	# sphinx expects one or more documents from xmlpipe stdin
	# each document must be formatted as follows:
	#
	# <document>
	# <id>123</id>
	# <group>45</group>
	# <timestamp>1132223498</timestamp>
	# <title>test title</title>
	# <body>
	# this is my document body
	# </body>
	# </document>
	#
	# timestamp element is optional, default value 1
	# all the other elements are mandatory

	# type				= xmlpipe
	# xmlpipe_command	= cat @CONFDIR@/test.xml
}


# inherited source example
#
# all the parameters are copied from the parent source,
# and may then be overridden in this source definition
source src1stripped : src1
{
	strip_html			= 1
}

#############################################################################
## index definition
#############################################################################

# local index example
#
# this is an index which is stored locally in the filesystem
# all indexing-time options (such as morphology and charsets) belong to the index
index test1
{
	# which document source to index
	# at least one MUST be defined
	#
	# multiple sources may be specified; to do so, just add more
	# "source = NAME" lines. in this case, ALL the document IDs
	# in ALL the specified sources MUST be unique
	source			= src1

	# this is path and index file name without extension
	# files <indexpath>.spi/spd/spr will be created by indexer
	#
	# .spr is temporary raw log, it can be removed when indexer is done
	# .spi/.spd are fulltext index files (index index and index data)
	#
	# MUST be defined
	path			= @CONFDIR@/data/test1
	# path			= @CONFDIR@/data/test1

	# docinfo (ie. per-document attribute values) storage strategy
	#
	# defines how docinfo will be stored
	# "none" means there'll be no docinfo (no groups, no dates, no nothing)
	# "inline" means that the docinfo will be stored along with the doc IDs
	# "extern" means that the docinfo will be stored separately
	#
	# externally stored docinfo should (basically) be kept in RAM
	# when querying; therefore, "inline" may be the only viable option
	# for really huge (50-100+ million docs) datasets. however, for
	# smaller datasets "extern" storage makes both indexing and
	# searching much more efficient.
	#
	# additional search-time memory requirements for extern storage are
	#
	#	( 1 + number_of_attrs )*number_of_docs*4 bytes
	#
	# so 10 million docs with 2 groups and 1 timestamp will take
	# (1+2+1)*10M*4 = 160 MB of RAM. this is per daemon, ie. searchd
	# will alloc 160 MB on startup, read the data and keep it shared
	# between queries.
	#
	# default is "extern" (as most collections are smaller than 100M docs)
	docinfo			= extern

	# morphology
	# default is not to use any
	#
	# currently supported morphology preprocessors are Porter stemmers
	# for English and Russian, and Soundex. more stemmers could be added
	# at users request.
	#
	# morphology		= none
	# morphology		= stem_en
	# morphology		= stem_ru
	# morphology		= stem_enru
	# morphology		= soundex
	morphology			= none

	# stopwords file
	# format is plain text in whatever encoding you use
	# optional, default is empty
	#
	# stopwords			= @CONFDIR@/data/stopwords.txt
	stopwords			=

	# minimum word length
	# only the words that are of this length and above will be indexed;
	# for example, if min_word_len is 4, "the" won't be indexed, but "they" will be.
	# default is 1, which (obviously) means to index everything
	min_word_len		= 1

	# charset encoding type
	# known types are 'sbcs' (Single Byte CharSet) and 'utf-8'
	# optional, default is sbcs
	charset_type		= sbcs

	# charset definition and case folding rules "table"
	# optional, default value depends on charset_type
	# for now, defaults are configured to support English and Russian
	# this behavior MAY change in future versions
	#
	# 'sbcs' default value is
	# charset_table		= 0..9, A..Z->a..z, _, a..z, U+A8->U+B8, U+B8, U+C0..U+DF->U+E0..U+FF, U+E0..U+FF
	#
	# 'utf-8' default value is
	# charset_table		= 0..9, A..Z->a..z, _, a..z, U+410..U+42F->U+430..U+44F, U+430..U+44F
}


# inherited index example
#
# all the parameters are copied from the parent index,
# and may then be overridden in this index definition
index test1stemmed : test1
{
	path			= @CONFDIR@/data/test1stemmed
	morphology		= stem_en
}


# distributed index example
#
# this is a virtual index which can NOT be directly indexed,
# and only containts references to other local and/or remote indexes
#
# if searchd receives a query against this index,
# it does the following:
#
# 1) connects to all the specified remote agents,
# 2) issues the query,
# 3) searches local indexes (while the remote agents are searching),
# 4) collects remote search results,
# 5) merges all the results together (removing the duplicates),
# 6) sends the merged resuls to client.
#
# this index type is primarily intenteded to be able to split huge (100GB+)
# datasets into chunks placed on different physical servers and them search
# through those chunks in parallel, reducing response times and server load;
# it seems, however, that it could also be used to take advantage of
# multi-processor systems or to implement HA (high-availability) search.
index dist1
{
	# 'distributed' index type MUST be specified
	type				= distributed

	# local index to be searched
	# there can be many local indexes configured
	local				= test1
	local				= test1stemmed

	# remote agent
	# multiple remote agents may be specified
	# syntax is 'hostname:port:index1,[index2[,...]]
	agent				= localhost:3313:remote1
	agent				= localhost:3314:remote2,remote3

	# remote agent connection timeout, milliseconds
	# optional, default is 1000 ms, ie. 1 sec
	agent_connect_timeout	= 1000

	# remote agent query timeout, milliseconds
	# optional, default is 3000 ms, ie. 3 sec
	agent_query_timeout		= 3000
}

#############################################################################
## indexer settings
#############################################################################

indexer
{
	# memory limit
	# can be specified in bytes, kilobytes (mem_limit=1000K) or megabytes (mem_limit=10M)
	# will grow if set unacceptably low
	# will warn if set too low, hurting the performance
	# optional, default is 32M
	mem_limit			= 32M
}

#############################################################################
## searchd settings
#############################################################################

searchd
{
	# port on which search daemon will listen
	port				= 3312


	# log file
	# searchd run info is logged here
	log					= @CONFDIR@/log/searchd.log


	# query log file
	# all the search queries are logged here
	query_log			= @CONFDIR@/log/query.log


	# client read timeout, seconds
	read_timeout		= 5


	# maximum amount of children to fork
	# useful to control server load
	max_children		= 30


	# a file which will contain searchd process ID
	# used for different external automation scripts
	# MUST be present
	pid_file			= @CONFDIR@/log/searchd.pid


	# maximum amount of matches this daemon would retrieve from each index
	# and serve to client
	#
	# this parameter affects per-client memory usage slightly (16 bytes per match)
	# and CPU usage in match sorting phase; so blindly raising it to 1 million
	# is definitely NOT recommended
	#
	# default is 1000 (just like with Google)
	max_matches			= 1000
}

# --eof--
